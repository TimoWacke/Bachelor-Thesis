\section{Process Orchestration + API \& Web interface}
\label{sec: process_orchestration}

As foretold in \autoref{sec: implementation_introduction}, the idea is to create a system that allows for efficient execution of the three different pipelines: Training, Infilling, and Validation.
With the foundation of the previous chapter we have all the necessary preprocessing and postprocessing routines in place.
When we succeed in creating the next abstraction layer, allowing for easy execution of the different pipelines, such that they handle their process autonomously, we can present an end-to-end solution.

The achieved end-to-end solution comes with a web interface, allowing for an easy user experience.
It gives the user everything they need to have control over the entire process, from submitting a dataset to downloading validation/training or infilling results.
Such a web interface requires defined touchpoints between the user's web browser and the server where the system is running.
A solution featuring programmatically defined touchpoints is called an API (Application Programming Interface), where a defined endpoint is available for each use case.
An overview of the API endpoints is given in \autoref{sec: api}. 

\subsection{Executor Classes: Training, Infilling, Validation}

The challenge for the foretold web interface and API solution is to store different station data independently and to control processes for each station dynamically.
This begins with passing the corresponding Station object to the executor class, which needs to initialize the process, including the creation of dedicated temporary folders.
The executors are designed as classes to allow for easy access to the different data of the process entity itself at all steps, from ERA5 download to plotting. For example, it ensures that when plotting after the CRAI module has finished the evaluation 

The \code{TrainingExecutor} class is structured such that all needed inputs are directly passed on object creation. However, the pipeline steps are initiated not from the class constructor but from an extra method called \code{execute}. This design decision allows for a similar \code{execute\_with\_sbatch} version that can be run instead. Using \code{sbatch} is useful when using the code stack on a supercomputer like Levante at the German Climate Computing Center.
The execution first runs the download steps for the ERA5 data which is a combination of a few routines described in \autoref{sec: implementation}: Identifying the available timesteps, downloading the data, merging the .grib files, converting to NetCDF files, and cropping in the space and time dimensions as described above.
This is where the benefit of the work in the previous chapter comes into play, as all of that is controlled by just a few lines and even undisturbed of temp-folder management which is directly included in the routines from \autoref{sec: implementation}.
After completion of the preprocessing steps, when the necessary coherent training files are saved in the temp-folders of the executor object, it generates the \code{training-args.txt} which is used as the input parameters for the separate machine learning code stack.
That file not only specifies how exactly the training should be executed and for how many iterations but also includes information about where the output, respectively the model files, should be saved.
As this is specified by one of the executor-objects properties, it is troublefree to handle the saving of the model wherever needed.

The \code{EvaluationExecutor} used for the infilling routine is structured similarly to the \code{TrainingExecutor}, but with the difference that the input parameters must include a model path, and that the download routines to obtain ERA5 data use the times where measurements are missing, instead of the times where measurements are available.
The class has an execute function as well which runs the different steps one after the other, but can be manipulated if certain steps such as the download of the ERA5 data should be run differently.
The \code{training-args.txt} file for the CRAI module is generated similarly but of course with the parameters needed for evaluation.

The Validation Executor is structured similarly to the \code{EvaluationExecutor}, but has the download routine of the \code{TrainingExecutor}, to obtain the ERA5 data for the times when measurements are available.
Also after the successful evaluation of the model, the plots for the comparison of the predictions with the actual measurements are generated and saved in the temp-folder of the executor object.
The plots also are generated comfortably with the plotting routine from \autoref{sec: implementation}.

\subsection{Web interface}

\begin{wrapfigure}[11]{r}{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{resources/images/webinterface_screenshot.png}
\caption{Screenshot of the webinterface}
\label{fig: webinterface_screenshot}
\end{wrapfigure}

As written in the Introduction of this chapter \ref{sec: process_orchestration}, the benefit of establishing the abstraction layer with the executor classes is that with minimal additional effort, API Endpoints for initiation of processes to train/validate a model as well as endpoints to retrieve the results after the processes have finished.
Additionally, it is possible to monitor the progress of the process through the API.
The details on the API are described in \autoref{sec: api}.
This lays out a sufficient foundation for the implementation of a web interface, which is the main focus of this section.

The interface consists of two areas, one where the user is supposed to submit a "dataset", which in this context is meant as a collection of measurements from a weather station and the station's information such as location, and optionally a custom name and third a trained model for the station that can be provided optionally as well.
If no model is provided the system can train a model and will attach it.

The second area is a list of all datasets that the user has submitted.
The API implementation allows for user identification through a unique token that is passed on to the server when the user submits a dataset, such that the user can then ask to see all datasets owned by them using that same token.
The web interface automatically generates a token for the user and stores it then in the local storage of the browser, such that the user does not have to remember it.

As examples in \autoref{fig: webinterface_screenshot} show each dataset depending on its state has different options available.
The deletion and train buttons all have in common, as no model is necessary to train a model.
Datasets where a model has been provided still could be used to train a new model overwriting the old one.
The \code{Evaluate} and \code{Fill in} buttons to evaluate the model over the timesteps where measurements are available, respectively to evaluate the model over the timesteps where measurements are missing.

Once a process such as training, evaluation, or infilling has been completed for a dataset the user can download the results anytime through the new buttons that appear in the dataset list.
For example, it can be seen in \autoref{fig: webinterface_screenshot} that for the Vienna example, validation has been completed, and the user can now download a PDF with all the plots that compare the predictions with the actual measurements, or a \code{CSV} file with the same data, or a zip file that contains both and the images used in the PDF.
The first Barbados example has a model attached that can be downloaded, meaning either it was provided or generated through training on the server itself.
The second Barbados example has no model attached yet, meaning only the buttons "Train" and "Delete" are available because no model was provided and no training has been done yet.

\subsection{API Endpoints}
\label{sec: api}

\subsubsection*{POST /data-submission}

Stores a new dataset on the server.
Each dataset is associated with a unique ID, and a unique token representing the owner.
The token of the owner is passed in the request body.
The dataset ID is created by the server and returned in the response.
The dataset ID is used to refer to the dataset in the following API calls.

\subsubsection*{GET /available-datasets/<user-token>}

For a given user token, this endpoint returns a list of all datasets that the user has submitted.
The user token is passed as a URL parameter.
The response is a list of dictionaries, each containing information about the dataset, such as the dataset ID, the name, and the status of the dataset (e.g. if it is busy training a model and progress percentage).

\subsubsection*{GET /train/<data-submission-id>}

\subsubsection*{GET /validate-model/<dataset-id>}

\subsubsection*{GET /fill-in/<dataset-id>}

\subsubsection*{DELETE /delete-dataset/<dataset-id>}

\subsubsection*{GET /download-model/<dataset-id>}

\subsubsection*{GET /download-validation-zip/<dataset-id>}

\subsubsection*{GET /download-infilling/<dataset-id>}